# 2.8 欠拟合和过拟合

## 学习目标

- 掌握过拟合、欠拟合的概念
- 掌握过拟合、欠拟合产生的原因
- 知道什么是正则化，以及正则化的分类

------



## 1 定义

- 过拟合：一个假设**在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据**，此时认为这个假设出现了过拟合的现象。(模型过于复杂)
- 欠拟合：一个假设**在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据**，此时认为这个假设出现了欠拟合的现象。(模型过于简单)

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2rlw69j315m0oc40y.jpg" alt="æ¬ æåè¿æåå¾ç¤º" style="zoom: 33%;" />

那么是什么原因导致模型复杂？线性回归进行训练学习的时候变成模型会变得复杂，这里就对应前面再说的线性回归的两种关系，非线性关系的数据，也就是存在很多无用的特征或者现实中的事物特征跟目标值的关系并不是简单的线性关系。

## 2 原因以及解决办法

- 欠拟合原因以及解决办法
    - 原因：学习到数据的特征过少
    - 解决办法：
        - **1）添加其他特征项，**有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。
        - **2）添加多项式特征**，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。
- 过拟合原因以及解决办法
    - 原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点
    - 解决办法：
        - 1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
        - 2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。
        - **3）正则化**
        - 4）减少特征维度，防止**维灾难**

## 3 正则化

### 3.1 什么是正则化

在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2sjcw9j314o0g8wkd.jpg" alt="æ¨¡åå¤æ" style="zoom:50%;" />

**如何解决？**

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2tduvuj30zs0kctav.jpg" alt="æ­£åå" style="zoom: 33%;" />

**在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化**

注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果

### 3.2 正则化类别

- L2正则化
    - 作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响
    - 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
    - Ridge回归
- L1正则化
    - 作用：可以使得其中一些W的值直接为0，删除这个特征的影响
    - LASSO回归



---

- 拓展阅读：【维灾难（见本章第14节）】


---



## 4 小结

- 欠拟合【掌握】
    - 在训练集上表现不好，在测试集上表现不好
    - 解决方法：
        - 继续学习
            - 1.添加其他特征项
            - 2.添加多项式特征
- 过拟合【掌握】
    - 在训练集上表现好，在测试集上表现不好
    - 解决方法：
        - 1.重新清洗数据集
        - 2.增大数据的训练量
        - 3.正则化
        - 4.减少特征维度
- 正则化【掌握】
    - 通过限制高次项的系数进行防止过拟合
    - L1正则化
        - 理解：直接把高次项前面的系数变为0
        - Lasso回归
    - L2正则化
        - 理解：把高次项前面的系数变成特别小的值
        - 岭回归