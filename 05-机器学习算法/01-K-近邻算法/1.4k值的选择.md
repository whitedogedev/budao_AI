# 1.4 k值的选择

## 学习目标

- 知道KNN中K值大小选择对模型的影响
- 知道估计误差和近似误差

------



## 1 K值选择说明

- **举例说明：**

![](https://tva1.sinaimg.cn/large/006tNbRwly1ga8u5omyzij30vc0d6gor.jpg)

- **K值过小**：
    - 容易受到异常点的影响
- **k值过大：**
    - 受到样本均衡的问题

---



**K值选择问题，李航博士的一书「统计学习方法」上所说：**

- 1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，
    - “学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，
    - 换句话说，**K值的减小就意味着整体模型变得复杂，容易发生过拟合；**
- 2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，
    - 其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，**与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误。**
    - **且K值的增大就意味着整体的模型变得简单。**
- 3) K=N（N为训练样本个数），则完全不足取，
    - 因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
- 在**实际应用中，K值一般取一个比较小的数值**，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。



---

- **近似误差**：
    - 对现有训练集的训练误差，**关注训练集**，
    - 如果**近似误差过小可能会出现过拟合的现象**，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。
    - 模型本身不是最接近最佳模型。
- **估计误差**：
    - 可以理解为对测试集的测试误差，关注测试集，
    - 估计误差小说明对未知数据的预测能力好，
    - 模型本身最接近最佳模型。

## 2 小结

- KNN中K值大小选择对模型的影响【知道】
    - **K值过小**：
        - 容易受到异常点的影响
        - 容易过拟合
    - **k值过大：**
        - 受到样本均衡的问题
        - 容易欠拟合
- 近似误差、估计误差基本概念介绍【了解】
    - 近似误差
        - 对现有训练集的训练误差，**关注训练集**
    - 估计误差
        - 可以理解为对测试集的测试误差，**关注测试集**



