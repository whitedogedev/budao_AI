# 6.5 算法优化

## 学习目标

- 知道k-means算法的优缺点
- 知道canopy、K-means++、二分K-means、K-medoids的优化原理
- 了解kernel K-means、ISODATA、Mini-batch K-means的优化原理

------



**k-means算法小结**

**优点：**

- 1.原理简单（靠近中心点），实现容易
- 2.聚类效果中上（依赖K的选择）
- 3.空间复杂度o(N)，时间复杂度o(I*K*N)

```
N为样本点个数，K为中心点个数，I为迭代次数
```

**缺点：**

- 1.对离群点，噪声敏感 （中心点易偏移）
- 2.很难发现大小差别很大的簇及进行增量计算
- 3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）

------

## 1 Canopy算法配合初始聚类

### 1.1 实现流程

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqs2sfaovj30u014b471.jpg" alt="image-20190219190832599" style="zoom: 50%;" />

### 1.2 Canopy算法优缺点

优点：

​	1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。    

​	2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。    

​	3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。

缺点：

​	1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解 

## 2 K-means++

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqs3bcr5wj30ho050dfu.jpg" alt="image-20190219233830941" style="zoom:50%;" />



> 其中：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqs3j3oalj30tc02o74a.jpg" alt="image-20190719205512954" style="zoom:50%;" />

为方便后面表示，把其记为A



![image-20190219233845360](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs43rcdij31ck09sq37.jpg)



![image-20190219233904862](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs470h4vj31d009a0t1.jpg)



![image-20190219233921494](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs49ga2uj31cy08g74m.jpg)

kmeans++目的，让选择的质心尽可能的分散

如下图中，如果第一个质心选择在圆心，那么最优可能选择到的下一个点在P(A)这个区域（根据颜色进行划分）

![image-20190219234135506](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs4crfwmj30ps0cyq4b.jpg)



## 3 二分k-means

实现流程:

- 1.所有点作为一个簇

- 2.将该簇一分为二

- 3.选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。

- 4.以此进行下去，直到簇的数目等于用户给定的数目k为止。

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqs4ghgtuj30na13k79e.jpg" alt="image-20190323000108301" style="zoom:50%;" />



**隐含的一个原则**

因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。

二分K均值算法可以加速K-means算法的执行速度，因为它的相似度计算少了并且不受初始化问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小



## 4 k-medoids（k-中心聚类算法）

K-medoids和K-means是有区别的，**不一样的地方在于中心点的选取**

- K-means中，将中心点取为当前cluster中所有数据点的平均值，对异常点很敏感!

- K-medoids中，将从当前cluster 中选取到其他所有（当前cluster中的）点的距离之和最小的点作为中心点。

    ![image-20190220000002208](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs4koc72j30ny08w75g.jpg)

算法流程：

-  ( 1 )总体n个样本点中任意选取k个点作为medoids

-  ( 2 )按照与medoids最近的原则，将剩余的n-k个点分配到当前最佳的medoids代表的类中

-  ( 3 )对于第i个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，代价函数的值，遍历所有可能，选取代价函数最小时对应的点作为新的medoids

-  ( 4 )重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数

-  ( 5 )产出最终确定的k个类


**k-medoids对噪声鲁棒性好。**

例：当一个cluster样本点只有少数几个，如（1,1）（1,2）（2,1）（1000,1000）。其中（1000,1000）是噪声。如果按照k-means质心大致会处在（1,1）（1000,1000）中间，这显然不是我们想要的。这时k-medoids就可以避免这种情况，他会在（1,1）（1,2）（2,1）（1000,1000）中选出一个样本点使cluster的绝对误差最小，计算可知一定会在前三个点中选取。



k-medoids只能对小样本起作用，样本大，速度就太慢了，当样本多的时候，少数几个噪音对k-means的质心影响也没有想象中的那么重，所以k-means的应用明显比k-medoids多。



## 5 Kernel k-means（了解）

kernel k-means实际上，就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。

![image-20190219235240810](https://tva1.sinaimg.cn/large/006tNbRwly1gaqs4o00q1j30k20aggne.jpg)


## 6 ISODATA（了解）

类别数目随着聚类过程而变化；

对类别数会进行合并，分裂，

“合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）

“分裂”：（当聚类结果中某一类的类内方差太大，将该类进行分裂）



## 7 Mini Batch K-Means（了解）

适合大数据的聚类算法

大数据量是什么量级？通常当样本量大于1万做聚类时，就需要考虑选用Mini Batch K-Means算法。

Mini Batch KMeans使用了Mini Batch（分批处理）的方法对数据点之间的距离进行计算。

Mini Batch计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本量少，所以会相应的减少运行时间，但另一方面抽样也必然会带来准确度的下降。

该算法的迭代步骤有两步：

- (1)从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心

- (2)更新质心


与Kmeans相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。



----

## 8 小结

- **k-means算法优缺点总结**【知道】
    - **优点：**
        - ​	1.原理简单（靠近中心点），实现容易
        - ​	2.聚类效果中上（依赖K的选择）
        - ​	3.空间复杂度o(N)，时间复杂度o(I*K*N)
    - **缺点：**
        - ​	1.对离群点，噪声敏感 （中心点易偏移）
        - ​	2.很难发现大小差别很大的簇及进行增量计算
        - ​	3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）



- 优化方法【知道】

| **优化方法**         | **思路**                     |
| -------------------- | ---------------------------- |
| Canopy+kmeans        | Canopy粗聚类配合kmeans       |
| kmeans++             | 距离越远越容易成为新的质心   |
| 二分k-means          | 拆除SSE最大的簇              |
| k-medoids            | 和kmeans选取中心点的方式不同 |
| kernel kmeans        | 映射到高维空间               |
| ISODATA              | 动态聚类，可以更改K值大小    |
| Mini-batch   K-Means | 大数据集分批聚类             |


