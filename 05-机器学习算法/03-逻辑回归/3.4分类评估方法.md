# 3.4 分类评估方法

## 学习目标

- 了解什么是混淆矩阵
- 知道分类评估中的精确率和召回率
- 知道roc曲线和auc指标

------



## 1.分类评估方法

### 1.1 精确率与召回率

#### 1.1.1 混淆矩阵

在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u0rhcrrj313s0m0jui.jpg" alt="image-20190321103913068" style="zoom:33%;" />

#### 1.1.2 精确率与召回率

- 精确率(Precision)：预测结果为正例样本中真实为正例的比例（了解）

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazpk71ocmj314q0mwafg.jpg" alt="image-20200117175505182" style="zoom:33%;" />



- 召回率(Recall)：真实为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力）

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gazpl17o7yj313a0mste1.jpg" alt="image-20200117175553201" style="zoom:33%;" />



### 1.2 F1-score

还有其他的评估标准，F1-score，反映了模型的稳健型

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u0ucp9nj313e082q5x.jpg" alt="image-20190321104006686" style="zoom: 33%;" />

----

### 1.3 分类评估报告api

- sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )
    - y_true：真实目标值
    - y_pred：估计器预测目标值
    - labels:指定类别对应的数字
    - target_names：目标类别名称
    - return：每个类别精确率与召回率

```python
ret = classification_report(y_test, y_predict, labels=(2,4), target_names=("良性", "恶性"))
print(ret)
```



**假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题**

问题：**如何衡量样本不均衡下的评估**？



## 2 ROC曲线与AUC指标

### 2.1 TPR与FPR

- TPR = TP / (TP + FN)
    - 所有真实类别为1的样本中，预测类别为1的比例
- FPR = FP / (FP + TN)
    - 所有真实类别为0的样本中，预测类别为1的比例

### 2.2 ROC曲线

- ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u0vxz69j30r80l20v8.jpg" alt="ROC" style="zoom:67%;" />

### 2.3 AUC指标

- AUC的概率意义是随机取一对正负样本，正样本得分大于负样本得分的概率
- AUC的范围在[0, 1]之间，并且越接近1越好，越接近0.5属于乱猜
- **AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。**
- **0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。**

### 2.4 AUC计算API

- from sklearn.metrics import roc_auc_score
    - sklearn.metrics.roc_auc_score(y_true, y_score)
        - 计算ROC曲线面积，即AUC值
        - y_true：每个样本的真实类别，必须为0(反例),1(正例)标记
        - y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值

```python
# 0.5~1之间，越接近于1约好
y_test = np.where(y_test > 2.5, 1, 0)

print("AUC指标：", roc_auc_score(y_test, y_predict)
```

- AUC只能用来评价二分类
- AUC非常适合评价样本不平衡中的分类器性能



------

##  3 小结

- 混淆矩阵【了解】
    - 真正例（TP）
    - 伪反例（FN）
    - 伪正例（FP）
    - 真反例（TN）
- 精确率(Precision)与召回率(Recall)【知道】
    - 准确率：（对不对）
        - （TP+TN）/(TP+TN+FN+FP)
    - 精确率 -- 查的准不准
        - TP/(TP+FP)
    - 召回率 -- 查的全不全
        - TP/(TP+FN)
    - F1-score
        - 反映模型的稳健性
- roc曲线和auc指标【知道】
    - roc曲线
        - 通过tpr和fpr来进行图形绘制，然后绘制之后，行成一个指标auc
    - auc
        - 越接近1，效果越好
        - 越接近0，效果越差
        - 越接近0.5，效果就是胡说
    - 注意：
        - 这个指标主要用于评价不平衡的二分类问题